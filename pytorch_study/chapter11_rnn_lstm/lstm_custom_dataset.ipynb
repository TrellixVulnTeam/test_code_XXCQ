{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample LSTM-based sentence label classification with custom dataset\n",
    "\n",
    "This is a simple LSTM code using custom dataset called \"Movie Review Sentiment Analysis\".\n",
    "It provides sentences of movie reviews and corresponding labels. See the dataset link for details.\n",
    "\n",
    "- Dataset link: https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data\n",
    "- Code reference: https://wangjiosw.github.io/2020/02/29/deep-learning/torchtext_use/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CUDA\n",
    "\n",
    "NOTE: sometimes when on a VSCode server IDE, pytorch cannot find GPU in a jupyter notebook code. However, it can find GPU if you run this python code from command line. Still no idea why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse raw dataset and split training and validation sets\n",
    "\n",
    "Dataset link: https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data.\n",
    "\n",
    "This dataset contains some tsv files (actually the same format of .csv files). I have already downloaded this dataset and put in `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (156060, 4), test.shape: (66292, 3)\n",
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  A series of escapades demonstrating the adage ...   \n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           A series   \n",
      "3         4           1                                                  A   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "   PhraseId  SentenceId                                             Phrase\n",
      "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
      "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
      "2    156063        8545                                                 An\n",
      "3    156064        8545  intermittently pleasing but mostly routine effort\n",
      "4    156065        8545         intermittently pleasing but mostly routine\n"
     ]
    }
   ],
   "source": [
    "# Read csv files first\n",
    "raw_train_data = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "raw_test_data = pd.read_csv('./data/test.tsv', sep='\\t')\n",
    "\n",
    "# Print some samples\n",
    "print('data.shape: {}, test.shape: {}'.format(\n",
    "    raw_train_data.shape, raw_test_data.shape))\n",
    "print(raw_train_data[:5])\n",
    "print(raw_test_data[:5])\n",
    "\n",
    "# Create train and validation set (both from training dataset)\n",
    "train_part, val_part = train_test_split(raw_train_data, test_size=0.2)\n",
    "# Need to save csv files. Will be loaded later to create data iterator.\n",
    "train_part.to_csv(\"./data/train_split.csv\", index=False)\n",
    "val_part.to_csv(\"./data/val_split.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary\n",
    "\n",
    "Steps:\n",
    "1. Create Field types TEXT and LABEL. \n",
    "2. Create Dataset objects for training, testing, validation;\n",
    "3. Build vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_vocab: 15396\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer. Two ways, both work:\n",
    "# 1) Define a function\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenizer(text):  # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "# 2) Use torchtext API.\n",
    "tokenizer_api = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Field\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer_api, lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# Dataset objects. Here we use API 'data.TabularDataset' which defines a dataset for CSV, TSV or json format.\n",
    "# Official Ref: https://torchtext.readthedocs.io/en/latest/data.html#tabulardataset\n",
    "# NOTE for parameters:\n",
    "# - If your csv file has a header row, remember to set 'skip_header=True'.\n",
    "# - In 'fields', the order must be exactly the same as order in csv files.\n",
    "# - You can also define your own Dataset class instead of using Torchtext API. See\n",
    "#   a ref here: https://blog.nowcoder.net/n/3a8d2c1b05354f3b942edfd4966bb0c1.\n",
    "\n",
    "train, val = data.TabularDataset.splits(\n",
    "    path='.', train='./data/train_split.csv', validation='./data/val_split.csv',\n",
    "    format='csv', skip_header=True,\n",
    "    fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT), ('Sentiment', LABEL)])\n",
    "\n",
    "test = data.TabularDataset(path='./data/test.tsv', format='tsv', skip_header=True,\n",
    "                           fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT)])\n",
    "\n",
    "# Build vocabulary from words in input data. This will also save the indices of words inside \n",
    "# the vocabulary (stored inside 'train' object). NOTE that words in vocabulary will be sorted\n",
    "# by word frequency. \n",
    "# - 'glove.6B.100d' is one encoding method provided by torchtext. Here 100d means each word is\n",
    "#   encoded as a 100-vector. This is like some word-to-encoding dictionary and will be\n",
    "#   downloaded to cache for the first time (about 900M).\n",
    "# - 'max_size' is to constrain the total number of encoded words. Other words not in this\n",
    "#   dictionary will be encoded to some default initialized vectors. This is reasonable to save\n",
    "#   space and time, since common words are not too many. If this parameter is not used,\n",
    "#   it will create vocabulary for all words met in the training data.\n",
    "#   NOTE that if this parameter is provided (such as 30000), and identical words is > 30000,\n",
    "#   then vocabulary's size will be actually 30002 instead of 30000, with 1 index for unmet\n",
    "#   words and another 1 for nothing (not very sure).\n",
    "TEXT.build_vocab(train, vectors='glove.6B.100d', max_size=30000)\n",
    "\n",
    "# This is to provide some initialization way for words not in dictionary.\n",
    "# 'xavier_uniform' is some method in a 2010 paper.\n",
    "# Official Ref: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_\n",
    "TEXT.vocab.vectors.unk_init = nn.init.xavier_uniform\n",
    "len_vocab = len(TEXT.vocab)\n",
    "print('len_vocab:', len_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['an', 'intermittently', 'pleasing', 'but', 'mostly', 'routine', 'effort', '.']\n"
     ]
    }
   ],
   "source": [
    "print(type(test.examples))\n",
    "print(test.examples[0].Phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create iterators\n",
    "\n",
    "An iterator is to:\n",
    "- 将 Datasets 中的数据 batch 化\n",
    "- 其中会包含一些 pad 操作，保证一个 batch 中的 example长度一致\n",
    "- 在这里将 string token 转化成 index\n",
    "\n",
    "Torchtext provides many iterator types. Here we use BucketIterator which:\n",
    "- Defines an iterator that batches examples of similar lengths together.\n",
    "- Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch.\n",
    "\n",
    "BucketIterator：相比于标准迭代器，会将类似长度的样本当做一批来处理，因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度。因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。\n",
    "\n",
    "除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old', '-', 'world']\n",
      "torch.Size([43, 128])\n",
      "torch.Size([128])\n",
      "tensor([2, 2, 2, 3, 2, 3, 2, 2, 1, 2, 1, 2, 3, 0, 1, 1, 3, 3, 0, 2, 1, 2, 2, 2,\n",
      "        2, 3, 1, 2, 2, 3, 2, 3, 1, 2, 2, 2, 2, 1, 4, 3, 1, 2, 3, 1, 2, 2, 2, 2,\n",
      "        1, 2, 3, 2, 1, 1, 1, 1, 2, 2, 4, 1, 2, 2, 1, 1, 4, 3, 2, 2, 1, 2, 3, 2,\n",
      "        2, 1, 2, 2, 2, 2, 4, 3, 2, 2, 3, 2, 2, 2, 2, 1, 1, 4, 3, 3, 2, 1, 3, 4,\n",
      "        2, 4, 4, 2, 1, 3, 1, 2, 3, 2, 1, 3, 0, 1, 3, 2, 2, 3, 2, 3, 2, 2, 3, 2,\n",
      "        3, 0, 2, 3, 3, 1, 4, 2])\n",
      "tensor([[  53,   46,   15,  ..., 3462,   52,   41],\n",
      "        [  64,    2,   81,  ...,    1,  199,   13],\n",
      "        [   5,  233, 2707,  ...,    1,    1, 1169],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Iterator. \n",
    "# 1.将 Datasets 中的数据 batch 化\n",
    "# 2.其中会包含一些 pad 操作，保证一个 batch 中的 example长度一致\n",
    "# 3.在这里将 string token 转化成 index，即 numericalization 过程 (应该是利用了此前建的 vocabulary，\n",
    "#   不过这里并没有 vocab 相关的输入参数。个人猜想是 'train' 内已经存储了每个单词在 vocabulary 中的 index)\n",
    "batch_size = 128\n",
    "train_iter = data.BucketIterator(train, batch_size=batch_size, sort_key=lambda x: len(x.Phrase),\n",
    "                                 shuffle=True, device=DEVICE)\n",
    "\n",
    "val_iter = data.BucketIterator(val, batch_size=batch_size, sort_key=lambda x: len(x.Phrase),\n",
    "                               shuffle=True, device=DEVICE)\n",
    "\n",
    "# NOTE: 在 test_iter, sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序。\n",
    "test_iter = data.Iterator(dataset=test, batch_size=batch_size, train=False,\n",
    "                          sort=False, device=DEVICE)\n",
    "\n",
    "# print(type(train.examples))\n",
    "print(val.examples[0].Phrase)\n",
    "\n",
    "# Print some sample\n",
    "first_batch = next(iter(train_iter))\n",
    "first_batch_data = first_batch.Phrase\n",
    "first_batch_target = first_batch.Sentiment\n",
    "print(first_batch_data.shape)\n",
    "print(first_batch_target.shape)\n",
    "print(first_batch_target)\n",
    "# The data should be integer indices of words, instead of original words.\n",
    "print(first_batch_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124848\n",
      "976\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(list(train_iter)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition\n",
    "\n",
    "Here we define a simple LSTM-based network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, num_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        # Create this 'lookup' table. NOTE that this is only initialization. You need to\n",
    "        # copy the vocabulary data to this variable outside this network class explicitly.\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            num_layers=num_layers, bidirectional=True, dropout=0.5)\n",
    "\n",
    "        # Here '* 2' because we are using bidirctional LSTM\n",
    "        self.linear = nn.Linear(hidden_dim * num_layers * 2, num_labels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [seq_len, b]\n",
    "        \"\"\"\n",
    "        # x: [seq_len, b] => embedding [seq_len, b, embedding_dim]\n",
    "        # nn.Embedding(x) is basically a lookup table, so for each element in x,\n",
    "        # it will replace this element with the embedding vector found in the table.\n",
    "        # This is exactly equal to adding a new dimension in the rightmost position of its size.\n",
    "        # That is,  [seq_len, b] => [seq_len, b, embedding_dim], while the latter is\n",
    "        # exactly the default input size of a LSTM network.\n",
    "        # NOTE:\n",
    "        # - This is interesting: Dropout can work on a common tensor (this is not like an unknown\n",
    "        # variable like weights). This is doable.\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        # out: [seq_len, b, hidden_dim]\n",
    "        # h: [num_layers * 2, b, hidden_dim] (here multiplying 2 because we are using bidirctional LSTM)\n",
    "        # c: [num_layers * 2, b, hidden_dim]\n",
    "        net_out, (h, c) = self.lstm(embedding)\n",
    "        # print('net_out: {}, h: {}, c: {}'.format(net_out.shape, h.shape, c.shape))\n",
    "\n",
    "        # [num_layers * 2, b, hidden_dim] => a tuple of [b, hidden_dim] with size 'num_layers * 2'\n",
    "        h_split = h.split(1, dim=0)  # tuple of [1, b, hidden_dim]\n",
    "        h_split = [x.squeeze(0) for x in h_split]  # tuple of [b, hidden_dim]\n",
    "        # print(h_split[0].shape)\n",
    "\n",
    "        # concatenate this list of [b, hidden_dim] => [b, hidden_dim * num_layers * 2]\n",
    "        h_cat = torch.cat(h_split, dim=1)\n",
    "        # print(h_cat.shape)\n",
    "\n",
    "        # [b, hidden_dim * num_layers * 2] => [b, num_labels]\n",
    "        out = self.linear(self.dropout(h_cat))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training preparation\n",
    "\n",
    "Here we create network-related variables, and copy vocabulary lookup table weights into network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_vectors: torch.Size([15396, 100])\n",
      "Embedding layer inited.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyLSTM(\n",
       "  (embedding): Embedding(15396, 100)\n",
       "  (lstm): LSTM(100, 256, num_layers=3, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=1536, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 5 labels in this dataset.\n",
    "net = MyLSTM(vocab_size=len_vocab, embedding_dim=100,\n",
    "             hidden_dim=256, num_labels=5, num_layers=3)\n",
    "\n",
    "# NOTE: copy vocabulary lookup table to embedding in the network.\n",
    "vocab_vectors = TEXT.vocab.vectors\n",
    "print('vocab_vectors:', vocab_vectors.shape)\n",
    "net.embedding.weight.data.copy_(vocab_vectors)\n",
    "print('Embedding layer inited.')\n",
    "\n",
    "net.to(DEVICE)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "criteon = nn.CrossEntropyLoss().to(DEVICE)\n",
    "net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/124848 (0%)]\tLoss: 1.616035\n",
      "Train Epoch: 0 [1280/124848 (1%)]\tLoss: 1.264598\n",
      "Train Epoch: 0 [2560/124848 (2%)]\tLoss: 1.247621\n",
      "Train Epoch: 0 [3840/124848 (3%)]\tLoss: 1.258708\n",
      "Train Epoch: 0 [5120/124848 (4%)]\tLoss: 1.193643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4179912/1420153941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/msyn/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/msyn/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/msyn/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_num = len(list(train_iter))\n",
    "sentence_num = len(train)\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    net.train()\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        source = batch.Phrase\n",
    "        target = batch.Sentiment\n",
    "        logits = net(source)\n",
    "        # print(logits.shape)\n",
    "\n",
    "        loss = criteon(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            curr_sentence_num = batch_idx * batch_size\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, curr_sentence_num, sentence_num,\n",
    "                100. * curr_sentence_num / sentence_num, loss.item()))\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for batch in val_iter:\n",
    "            source = batch.Phrase\n",
    "            target = batch.Sentiment\n",
    "            logits = net(source)\n",
    "            # print(logits.shape)\n",
    "\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum()\n",
    "\n",
    "        test_loss /= len(val)\n",
    "        accuracy = 100. * correct / len(val)\n",
    "        print(\n",
    "            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(val), accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78c90db3fdec11e63d5d3069d521b560b86c30c1734f467cffa85a08d0412b0b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('msyn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
