{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample LSTM-based sentence label classification with custom dataset\n",
    "\n",
    "This is a simple LSTM code using custom dataset called \"Movie Review Sentiment Analysis\".\n",
    "It provides sentences of movie reviews and corresponding labels. See the dataset link for details.\n",
    "\n",
    "- Dataset link: https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data\n",
    "- Code reference: https://wangjiosw.github.io/2020/02/29/deep-learning/torchtext_use/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check CUDA\n",
    "\n",
    "NOTE: sometimes when on a VSCode server IDE, pytorch cannot find GPU in a jupyter notebook code. However, it can find GPU if you run this python code from command line. Still no idea why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse raw dataset and split training and validation sets\n",
    "\n",
    "Dataset link: https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data.\n",
    "\n",
    "This dataset contains some tsv files (actually the same format of .csv files). I have already downloaded this dataset and put in `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (156060, 4), test.shape: (66292, 3)\n",
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  A series of escapades demonstrating the adage ...   \n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           A series   \n",
      "3         4           1                                                  A   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "   PhraseId  SentenceId                                             Phrase\n",
      "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
      "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
      "2    156063        8545                                                 An\n",
      "3    156064        8545  intermittently pleasing but mostly routine effort\n",
      "4    156065        8545         intermittently pleasing but mostly routine\n"
     ]
    }
   ],
   "source": [
    "# Read csv files first\n",
    "raw_train_data = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "raw_test_data = pd.read_csv('./data/test.tsv', sep='\\t')\n",
    "\n",
    "# Print some samples\n",
    "print('data.shape: {}, test.shape: {}'.format(\n",
    "    raw_train_data.shape, raw_test_data.shape))\n",
    "print(raw_train_data[:5])\n",
    "print(raw_test_data[:5])\n",
    "\n",
    "# Create train and validation set (both from training dataset)\n",
    "train_part, val_part = train_test_split(raw_train_data, test_size=0.2)\n",
    "# Need to save csv files. Will be loaded later to create data iterator.\n",
    "train_part.to_csv(\"./data/train_split.csv\", index=False)\n",
    "val_part.to_csv(\"./data/val_split.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary\n",
    "\n",
    "Steps:\n",
    "1. Create Field types TEXT and LABEL. \n",
    "2. Create Dataset objects for training, testing, validation;\n",
    "3. Build vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_vocab: 15395\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer. Two ways, both work:\n",
    "# 1) Define a function\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenizer(text):  # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "# 2) Use torchtext API.\n",
    "tokenizer_api = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Field\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer_api, lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# Dataset objects. Here we use API 'data.TabularDataset' which defines a dataset for CSV, TSV or json format.\n",
    "# Official Ref: https://torchtext.readthedocs.io/en/latest/data.html#tabulardataset\n",
    "# NOTE for parameters:\n",
    "# - If your csv file has a header row, remember to set 'skip_header=True'.\n",
    "# - In 'fields', the order must be exactly the same as order in csv files.\n",
    "# - You can also define your own Dataset class instead of using Torchtext API. See\n",
    "#   a ref here: https://blog.nowcoder.net/n/3a8d2c1b05354f3b942edfd4966bb0c1.\n",
    "\n",
    "train, val = data.TabularDataset.splits(\n",
    "    path='.', train='./data/train_split.csv', validation='./data/val_split.csv',\n",
    "    format='csv', skip_header=True,\n",
    "    fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT), ('Sentiment', LABEL)])\n",
    "\n",
    "test = data.TabularDataset(path='./data/test.tsv', format='tsv', skip_header=True,\n",
    "                           fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT)])\n",
    "\n",
    "# -- Build vocabulary from words in input data. NOTE that words in vocabulary will be sorted\n",
    "# by word frequency.\n",
    "# - 'glove.6B.100d' is one encoding method provided by torchtext. Here 100d means each word is\n",
    "#   encoded as a 100-vector. This is like some word-to-encoding dictionary and will be\n",
    "#   downloaded to cache for the first time (about 900M).\n",
    "# - 'max_size' is to constrain the total number of encoded words. Other words not in this\n",
    "#   dictionary will be encoded to some default initialized vectors. This is reasonable to save\n",
    "#   space and time, since common words are not too many. If this parameter is not used,\n",
    "#   it will create vocabulary for all words met in the training data.\n",
    "#   NOTE that if this parameter is provided (such as 30000), and identical words is > 30000,\n",
    "#   then vocabulary's size will be actually 30002 instead of 30000, with 1 index for unmet\n",
    "#   words and another 1 for nothing (not very sure).\n",
    "TEXT.build_vocab(train, vectors='glove.6B.100d', max_size=30000)\n",
    "\n",
    "# This is to provide some initialization way for words not in dictionary.\n",
    "# 'xavier_uniform' is some method in a 2010 paper.\n",
    "# Official Ref: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_\n",
    "TEXT.vocab.vectors.unk_init = nn.init.xavier_uniform\n",
    "len_vocab = len(TEXT.vocab)\n",
    "print('len_vocab:', len_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create iterators\n",
    "\n",
    "Torchtext provides many iterator types. Here we use BucketIterator which:\n",
    "- Defines an iterator that batches examples of similar lengths together.\n",
    "- Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 128])\n",
      "tensor([[   4, 1604,  106,  ..., 1182, 6145,    4],\n",
      "        [  74, 3563, 2065,  ...,   62, 4904, 2265],\n",
      "        [9986,   16,    1,  ..., 1434,  226,    1],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/tiger/miniconda3/envs/msyn/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Iterator\n",
    "train_iter = data.BucketIterator(train, batch_size=128, sort_key=lambda x: len(x.Phrase),\n",
    "                                 shuffle=True, device=DEVICE)\n",
    "\n",
    "val_iter = data.BucketIterator(val, batch_size=128, sort_key=lambda x: len(x.Phrase),\n",
    "                               shuffle=True, device=DEVICE)\n",
    "\n",
    "# NOTE: 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序\n",
    "test_iter = data.Iterator(dataset=test, batch_size=128, train=False,\n",
    "                          sort=False, device=DEVICE)\n",
    "\n",
    "\n",
    "# Print some sample\n",
    "first_batch = next(iter(train_iter))\n",
    "data = first_batch.Phrase\n",
    "label = first_batch.Sentiment\n",
    "print(first_batch.Phrase.shape)\n",
    "print(first_batch.Phrase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition\n",
    "\n",
    "Here we define a simple LSTM-based network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, num_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        # Create this 'lookup' table. NOTE that this is only initialization. You need to\n",
    "        # copy the vocabulary data to this variable outside this network class explicitly.\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            num_layers=num_layers, bidirectional=True, dropout=0.5)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_dim * num_layers, num_labels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [seq_len, b, v]\n",
    "        \"\"\"\n",
    "        # NOTE for this interesting thing: we add a Dropout on original encoded data x.\n",
    "        # This is doable.\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        # out: [seq_len, b, hidden_dim]\n",
    "        # h: [num_layers, b, hidden_dim]\n",
    "        # c: [num_layers, b, hidden_dim]\n",
    "        net_out, (h, c) = self.lstm(embedding)\n",
    "\n",
    "        # [num_layers, b, hidden_dim] => a list of [b, hidden_dim] with size 'num_layers'\n",
    "        h_split = h.split(h.size(0), dim=0)\n",
    "\n",
    "        # concatenate this list of [b, hidden_dim] => [b, hidden_dim * num_layers]\n",
    "        h_cat = torch.cat(h_split, dim=1)\n",
    "\n",
    "        # [b, hidden_dim * num_layers] => [b, num_labels]\n",
    "        out = self.linear(self.dropout(h_cat))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training preparation\n",
    "\n",
    "Here we create network-related variables, and copy vocabulary lookup table weights into network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 5 labels in this dataset.\n",
    "net = MyLSTM(len_vocab, embedding_dim=100,\n",
    "             hidden_dim=256, num_labels=5, num_layers=3)\n",
    "\n",
    "# NOTE: copy vocabulary lookup table to embedding in the network.\n",
    "vocab_vectors = TEXT.vocab.vectors\n",
    "print('vocab_vectors:', vocab_vectors.shape)\n",
    "net.embedding.weight.data.copy_(vocab_vectors)\n",
    "print('Embedding layer inited.')\n",
    "\n",
    "net.to(DEVICE)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "criteon = nn.CrossEntropyLoss().to(DEVICE)\n",
    "net.to(DEVICE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78c90db3fdec11e63d5d3069d521b560b86c30c1734f467cffa85a08d0412b0b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('msyn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
